{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day01 数据采集基础"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1、爬虫概念"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 什么是爬虫"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 生活角度：\n",
    "\n",
    "\t爬虫，蜘蛛（spider），网\n",
    "\n",
    "- 互联网\n",
    "\n",
    "\t网：互联网，上面的节点就是很多的url（统一资源定位符）\n",
    "\n",
    "- 互联网爬虫：\n",
    "\n",
    "\t就是写一个程序，就是根据url用来爬取网页，然后将网页中的你所需要的数据提取出来都有哪些语言可以实现爬虫"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 用什么爬虫"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- php：\n",
    "\n",
    "    号称是世界最优美的语言，但是他不是很擅长这个，对多进程多线程支持的不好\n",
    "- java：\n",
    "\n",
    "    做起来也非常的不错，是python爬虫最主要的对手，代码太臃肿，代码量很大，重构成本非常的大，而我们爬虫需要根据需求经常修改，所以它不好\n",
    "- c、c++：\n",
    "    \n",
    "    学习成本比较高，性能和效率非常高，没这么做的，仅仅是一个能力的体现\n",
    "- python：\n",
    "\n",
    "    好，语法优美、代码简单，学习成本低，支持的模块多，有一个非常强大的爬虫框架，scrapy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 通用爬虫和聚焦爬虫概念"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 通用爬虫"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| 百度、360、谷歌、搜狗、必应等搜索引擎"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "搜索引擎使用的爬虫就是通用爬虫\n",
    "\t（1）抓取网页\n",
    "\t（2）抓取数据\n",
    "\t（3）数据存储\n",
    "\t（4）数据处理\n",
    "\t（5）给你提供了检索服务\n",
    "抓取流程：\n",
    "\t（1）给一些起始的url，放入待爬取url队列\n",
    "\t（2）从队列中取出url，开始爬取\n",
    "\t（3）分析内容，获取网页中所有的url，继续执行第二步，直到结束\n",
    "搜索引擎如何获取一个新的网站的链接\n",
    "\t（1）主动给搜索引擎提交url\n",
    "\t（2）在其它网站中设置友情链接\n",
    "\t（3）百度和DNS服务商进行合作，加速收录新网站\n",
    "robots协议\n",
    "\t淘宝就不让百度抓取\n",
    "\t可以限制通用爬虫的抓取，哪些可以抓，哪些不能抓\n",
    "\t仅仅是一个协议，一般情况只有大型搜索引擎遵从这个协议，你自己写的小爬虫，就算了，你可以随便抓取\n",
    "网站排名（SEO）\n",
    "\t（1）根据pagerank值排名，根据流量、点击率等等进行综合的计算进行排名，值越高，排名越靠前\n",
    "\t（2）百度竞价排名，谁给的钱多，谁在最前面\n",
    "    缺点：\n",
    "\t（1）抓取的很多数据都是无用的\n",
    "\t（2）不能根据用户的需求来抓取对应的数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "聚焦爬虫\n",
    "\t根据自己的需求，去写一个网络爬虫程序，抓取对应的数据即可"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "爬虫如何抓取网页数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "网页都有特点：\n",
    "\t（1）网页都有自己的唯一的统一资源定位符（url）\n",
    "\t（2）网页都是有html组成的\n",
    "\t（3）传输协议使用的都是http、https协议\n",
    "爬虫设计的思路是：\n",
    "\t（1）给我一个url\n",
    "\t（2）模拟浏览器通过http协议访问url，获取到这个url的html代码\n",
    "\t（3）解析字符串（根据一定规则提取你所需要的数据）\n",
    "开发环境：\n",
    "\tpython3.6\n",
    "\tsublime   pycharm\n",
    "整体内容：\n",
    "\t1、python语法\n",
    "\t2、如何抓取页面，使用到python库\n",
    "\t\turllib.reqeust  urllib.parse  requests\n",
    "\t3、解析内容\n",
    "\t\t正则表达式、xpath、bs4、jsonpath\n",
    "\t4、采集动态html\n",
    "\t\tselenium+phantomjs\n",
    "\t5、scrapy\n",
    "\t\t高性能异步网络框架\n",
    "\t6、分布式，scrapy-redis组件\n",
    "\t\t在scrapy的基础上增了一套组件，结合redis进行存储等功能\n",
    "\t7、爬虫-反爬虫-反反爬虫之间的博弈过程\n",
    "\t\t其实爬虫到最后，让你头疼的不是复杂的界面，不是数据的提取，而是和对面相互博弈的过程\n",
    "\t\t反爬虫的一般手段：\n",
    "\t\t\tUser-Agent、代理、验证码、动态数据加载、数据加密\n",
    "\t\t最终肯定能获取数据，公司值不值得，因为只要浏览器能够正常访问，那么数据就能拿到"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2、http协议"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\thttp就是应用层的协议，https\n",
    "\thttp：明文传输   80\n",
    "\thttps：加密传输  443 \n",
    "\tftp：21\n",
    "\tssh：22\n",
    "\tmysql：3306\n",
    "\tredis：6379\n",
    "\tMongoDB：27017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http工作原理："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "url详解：\n",
    "\t\thttp://www.taobao.com:80/index.html?username=goudan&password=123#anchor\n",
    "\t\t协议   主机           端口号  路径  query-string（参数）         锚点\n",
    "\t\t一张网页的程序有至少10-20个请求，每一个css、js、图片都是一个http请求\n",
    "\t\t浏览器就会将html、css、js、img翻译成图文并茂的形式"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http请求和响应\n",
    "\t\t请求行、请求头、请求体\n",
    "\t\t\tget、post\n",
    "\t\t响应行、响应头、响应体"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "请求头\n",
    "\t\t发送请求的时候，告诉服务器可以接受那些内容，MIME\n",
    "\t\tAccept:text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8\n",
    "\t\n",
    "\t\t客户端可以接受的编码类型\n",
    "\t\tAccept-Encoding:gzip, deflate, br\n",
    "\t\n",
    "\t\t接受的语言类型\n",
    "\t\tAccept-Language:zh-CN,zh;q=0.9\n",
    "\t\n",
    "\t\t和缓存相关\n",
    "\t\tCache-Control:max-age=0\n",
    "\t\n",
    "\t\t连接方式，保持长连接\n",
    "\t\tConnection:keep-alive\n",
    "\t\n",
    "\t\t会话相关\n",
    "\t\tCookie:BIDUPSID=9F5816DD3088F4291EA4C12FFC2ABCDE; BAIDUID=78AF1CE91C8F84BD601F6E2778C618DA:FG=1; PSTM=1513827071; BD_UPN=12314353; H_PS_PSSID=1454_21125_18559_25177; BD_CK_SAM=1; PSINO=2; BDORZ=B490B5EBF6F3CD402E515D22BCDA1598; pgv_pvi=769396736; pgv_si=s5038345216; BD_HOME=0; H_PS_645EC=3ff96rRFDTjHDZ%2BqFcj5%2FYOP6KISjZyhvnS1a%2B1q4A19NgE%2FSjllKIn8ejA\n",
    "\t\n",
    "\t\t主机\n",
    "\t\tHost:www.baidu.com\n",
    "\t\n",
    "\t\t是否升级为https请求\n",
    "\t\tUpgrade-Insecure-Requests:1\n",
    "\t\n",
    "\t\t客户端浏览器类型\n",
    "\t\tUser-Agent:Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/63.0.3239.108 Safari/537.36\n",
    "\t\n",
    "\t\t如果是ajax请求，一般都带这个\n",
    "\t\tX-Requested-With: XMLHttpRequest\n",
    "\t\n",
    "\t\t上一个页面，你从哪个页面过来的\n",
    "\t\tReferer: https://www.baidu.com/?tn=57095150_6_oem_dg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "响应头信息\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\t\tConnection:Keep-Alive\n",
    "\t\n",
    "\t\t内容编码格式\n",
    "\t\tContent-Encoding:gzip\n",
    "\t\t内容类型\n",
    "\t\tContent-Type:text/html; charset=utf-8\n",
    "\t\n",
    "\t\t时间\n",
    "\t\tDate:Sun, 24 Dec 2017 04:21:28 GMT\n",
    "\t\t过期时间\n",
    "\t\tExpires:Sun, 24 Dec 2017 04:20:29 GMT\n",
    "\t\t服务器版本\n",
    "\t\tServer:BWS/1.1\n",
    "\t\t给客户端保存的cookie值\n",
    "\t\tSet-Cookie:BDSVRTM=0; path=/\n",
    "\t\tSet-Cookie:BD_HOME=0; path=/\n",
    "\t\tSet-Cookie:H_PS_PSSID=1454_21125_18559_25177; path=/; domain=.baidu.com\n",
    "\t\n",
    "\t\t内容是否分块传输\n",
    "\t\tTransfer-Encoding:chunked"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "常见的http状态码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['未命名.ipynb', '.ipynb_checkpoints']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.listdir('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib import request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AbstractBasicAuthHandler',\n",
       " 'AbstractDigestAuthHandler',\n",
       " 'AbstractHTTPHandler',\n",
       " 'BaseHandler',\n",
       " 'CacheFTPHandler',\n",
       " 'ContentTooShortError',\n",
       " 'DataHandler',\n",
       " 'FTPHandler',\n",
       " 'FancyURLopener',\n",
       " 'FileHandler',\n",
       " 'HTTPBasicAuthHandler',\n",
       " 'HTTPCookieProcessor',\n",
       " 'HTTPDefaultErrorHandler',\n",
       " 'HTTPDigestAuthHandler',\n",
       " 'HTTPError',\n",
       " 'HTTPErrorProcessor',\n",
       " 'HTTPHandler',\n",
       " 'HTTPPasswordMgr',\n",
       " 'HTTPPasswordMgrWithDefaultRealm',\n",
       " 'HTTPPasswordMgrWithPriorAuth',\n",
       " 'HTTPRedirectHandler',\n",
       " 'HTTPSHandler',\n",
       " 'MAXFTPCACHE',\n",
       " 'OpenerDirector',\n",
       " 'ProxyBasicAuthHandler',\n",
       " 'ProxyDigestAuthHandler',\n",
       " 'ProxyHandler',\n",
       " 'Request',\n",
       " 'URLError',\n",
       " 'URLopener',\n",
       " 'UnknownHandler',\n",
       " '__all__',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__spec__',\n",
       " '__version__',\n",
       " '_cut_port_re',\n",
       " '_ftperrors',\n",
       " '_get_proxies',\n",
       " '_get_proxy_settings',\n",
       " '_have_ssl',\n",
       " '_localhost',\n",
       " '_noheaders',\n",
       " '_opener',\n",
       " '_parse_proxy',\n",
       " '_proxy_bypass_macosx_sysconf',\n",
       " '_randombytes',\n",
       " '_safe_gethostbyname',\n",
       " '_thishost',\n",
       " '_url_tempfiles',\n",
       " 'addclosehook',\n",
       " 'addinfourl',\n",
       " 'base64',\n",
       " 'bisect',\n",
       " 'build_opener',\n",
       " 'collections',\n",
       " 'contextlib',\n",
       " 'email',\n",
       " 'ftpcache',\n",
       " 'ftperrors',\n",
       " 'ftpwrapper',\n",
       " 'getproxies',\n",
       " 'getproxies_environment',\n",
       " 'getproxies_macosx_sysconf',\n",
       " 'hashlib',\n",
       " 'http',\n",
       " 'install_opener',\n",
       " 'io',\n",
       " 'localhost',\n",
       " 'noheaders',\n",
       " 'os',\n",
       " 'parse_http_list',\n",
       " 'parse_keqv_list',\n",
       " 'pathname2url',\n",
       " 'posixpath',\n",
       " 'proxy_bypass',\n",
       " 'proxy_bypass_environment',\n",
       " 'proxy_bypass_macosx_sysconf',\n",
       " 'quote',\n",
       " 're',\n",
       " 'request_host',\n",
       " 'socket',\n",
       " 'splitattr',\n",
       " 'splithost',\n",
       " 'splitpasswd',\n",
       " 'splitport',\n",
       " 'splitquery',\n",
       " 'splittag',\n",
       " 'splittype',\n",
       " 'splituser',\n",
       " 'splitvalue',\n",
       " 'ssl',\n",
       " 'string',\n",
       " 'sys',\n",
       " 'tempfile',\n",
       " 'thishost',\n",
       " 'time',\n",
       " 'to_bytes',\n",
       " 'unquote',\n",
       " 'unquote_to_bytes',\n",
       " 'unwrap',\n",
       " 'url2pathname',\n",
       " 'urlcleanup',\n",
       " 'urljoin',\n",
       " 'urlopen',\n",
       " 'urlparse',\n",
       " 'urlretrieve',\n",
       " 'urlsplit',\n",
       " 'urlunparse',\n",
       " 'warnings']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(request)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
